{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import ta\n",
    "#import talib\n",
    "\n",
    "from oandapyV20 import API\n",
    "from oandapyV20.contrib.factories import InstrumentsCandlesFactory\n",
    "import oandapyV20.endpoints.forexlabs as labs\n",
    "# https://media.readthedocs.org/pdf/oanda-api-v20/latest/oanda-api-v20.pdf\n",
    "# https://financetrain.com/best-python-librariespackages-finance-financial-data-scientists/\n",
    "# https://github.com/mrjbq7/ta-lib\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_loss = 0.0030\n",
    "take_profit = 0.0070\n",
    "periods = 5\n",
    "commision = 0.0002\n",
    "number_of_models_to_test = 10\n",
    "days_to_train = 5\n",
    "granularity_param = 'H1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = API(access_token='7f736aabc877f3ea75bc844c79814d7c-998e41725285ea6d54b836e8b93fe4f6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(api, instrument, start_days, end_days, granularity):\n",
    "\n",
    "    start_date = (dt.datetime.now()-dt.timedelta(days=start_days)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_date = (dt.datetime.now()-dt.timedelta(days=end_days, hours=2,minutes=4)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    params ={\n",
    "                \"from\": start_date,\n",
    "                \"to\": end_date,\n",
    "                \"granularity\":granularity,\n",
    "            }\n",
    "\n",
    "    df_list = []\n",
    "    for r in InstrumentsCandlesFactory(instrument=instrument,params=params):\n",
    "        api.request(r)\n",
    "        df = pd.DataFrame(r.response['candles'])\n",
    "        if(df.empty==False):\n",
    "            time = df['time']\n",
    "            volume = pd.DataFrame(df['volume'].apply(pd.Series))\n",
    "            df = pd.DataFrame(df['mid'].apply(pd.Series))\n",
    "            df = pd.concat([df,time,volume], axis=1)\n",
    "            df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%dT%H:%M:%S.000000000Z')\n",
    "            #df.set_index('time',inplace=True)\n",
    "            df_list.append(df)\n",
    "    \n",
    "    final = pd.concat(df_list)\n",
    "    \n",
    "    names = {\n",
    "        'o': 'open',\n",
    "        'c': 'close',\n",
    "        'h': 'high',\n",
    "        'l': 'low',\n",
    "        0: 'vol',\n",
    "        'time': 'time',\n",
    "    }\n",
    "    new_names = []\n",
    "    for column_name in final.columns:\n",
    "        new_names.append(names[column_name])\n",
    "    final.columns = new_names\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(client, instrument, perdiod):\n",
    "\n",
    "    \n",
    "    \n",
    "    params = {\n",
    "        \"instrument\": instrument,\n",
    "        \"period\": perdiod\n",
    "    }\n",
    "    \n",
    "    # PERIOD VALUES\n",
    "    #3600 - 1 hour\n",
    "    #43200 - 12 hours\n",
    "    #86400 - 1 day\n",
    "    #604800 - 1 week\n",
    "    #2592000 - 1 month\n",
    "    #7776000 - 3 months\n",
    "    #15552000 - 6 months\n",
    "    #31536000 - 1 year\n",
    "    # http://developer.oanda.com/rest-live/forex-labs/\n",
    "\n",
    "    r = labs.Calendar(params=params)\n",
    "    client.request(r)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(r.response, orient='columns')\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']*1000000000)\n",
    "    df = df[['impact', 'timestamp']]\n",
    "    df.columns = ['impact', 'time']\n",
    "\n",
    "    return df.groupby('time').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(history, calendar):\n",
    "    return pd.merge(history, calendar, left_on = 'time', right_on = 'time', how='outer')\\\n",
    "                                                                            .set_index('time')\\\n",
    "                                                                            .astype(float)\\\n",
    "                                                                            .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ta(df):\n",
    "    df = ta.add_all_ta_features(df, \"open\", \"high\", \"low\", \"close\", \"vol\", fillna=False)\n",
    "    \n",
    "    for feature in df.drop([\"open\", \"high\", \"low\", \"close\", \"vol\", \"impact\"], axis=1).columns.tolist():\n",
    "        df[feature+\"_change\"] = (df[feature] - df[feature].shift(1)) / df[feature].shift(1)\n",
    "        df = df.drop(feature, axis=1)\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\\\n",
    "            .dropna(axis=1, thresh=len(df) - 50)\\\n",
    "            .iloc[100:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broaden_impact(df, period):\n",
    "    df = df.reset_index().sort_values('time').set_index('time')\n",
    "\n",
    "    for i in range(periods):\n",
    "        df.loc[\n",
    "            (df['impact'].shift(-1-i) != 0)\n",
    "        ,'impact'] = df['impact'].shift(-1-i)\n",
    "    \n",
    "    df['impact'] = df['impact'].fillna(0)\n",
    "\n",
    "    return df[df['low'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sessions(df):\n",
    "    df.loc[df.index.hour.isin([7,8,9,10,11,12,13,14,15,16]), 'eu_session'] = 1\n",
    "    df.loc[df['eu_session'] != 1, 'eu_session'] = 0\n",
    "\n",
    "    df.loc[df.index.hour.isin([21,22,23,24,1,2,3,4,5,6,7]), 'asia_session'] = 1\n",
    "    df.loc[df['asia_session'] != 1, 'asia_session'] = 0\n",
    "\n",
    "    df.loc[df.index.hour.isin([12,13,14,15,16,17,18,19,20]), 'us_session'] = 1\n",
    "    df.loc[df['us_session'] != 1, 'us_session'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_change(df, periods):\n",
    "    df['change'] = (df['close'].shift(-periods) - df['close'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(df, plus_change, minus_change):\n",
    "    df.loc[df['change'] > plus_change,'label'] = 2\n",
    "    df.loc[df['change'] < -minus_change,'label'] = 1\n",
    "    df.loc[df['label'].isna(),'label'] = 0\n",
    "    \n",
    "    df = df.drop(['open', 'change'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, periods):\n",
    "    # friday evening\n",
    "    df = df[~((df.index.dayofweek == 4) & (df.index.hour >= (20-periods)))]\n",
    "    # calendar\n",
    "    df = df[df['impact'] == 0 ]\n",
    "    # monday's morning\n",
    "    df = df[~((df.index.dayofweek == 0) &( df.index.hour < 7))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_counter(df):\n",
    "    for i, date in enumerate(sorted(list(set(df.index.date)))):\n",
    "        df.loc[df.index.date == date ,'day_index'] = i\n",
    "        \n",
    "    df['weekday'] = df.index.dayofweek.values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = cal(client, 'EUR_USD', 7776000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hist(client, 'EUR_USD', 90, 0, granularity_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge(history, calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged[merged.index.duplicated(keep=False) == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged.index.duplicated(keep='first') == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = 3\n",
    "days_to_train = 5\n",
    "plus_change = 0.0010\n",
    "minus_change = 0.0010\n",
    "\n",
    "tree_criterions = ['entropy','gini']\n",
    "tree_max_depths = [6,8,10]\n",
    "tree_min_samples_leafs = [10, 25, 50]\n",
    "\n",
    "number_of_models_to_test = 1\n",
    "commision = 0.0002\n",
    "stop_loss = 0.0030\n",
    "take_profit = 0.0070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ta/trend.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (dip_mio[i]/trs[i])\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ta/trend.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (din_mio[i]/trs[i])\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ta/trend.py:634: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  aroon_up = close.rolling(n).apply(lambda x: float(np.argmax(x) + 1) / n * 100)\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ta/trend.py:656: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  aroon_down = close.rolling(n).apply(lambda x: float(np.argmin(x) + 1) / n * 100)\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:366: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(new_indexer, value)\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/jacekslusarczyk/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "with_broaden_impact = broaden_impact(merged, periods)\n",
    "with_ta_impact = add_ta(with_broaden_impact)\n",
    "with_ta_impact_sessions = add_sessions(with_ta_impact)\n",
    "with_ta_impact_sessions_change = add_change(with_ta_impact_sessions, periods)\n",
    "labeled = label(with_ta_impact_sessions_change, plus_change, minus_change)\n",
    "final = labeled.dropna(axis=1, how='any')\n",
    "final = add_day_counter(final)\n",
    "final = filter_data(final, periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    507\n",
       "1.0    157\n",
       "2.0    189\n",
       "Name: vol, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.groupby('label').count()['vol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the best parameters combination\n",
    "So for each parameters train model on past X days, chec performance at next day\n",
    "save the scores for each day and summarize them at the end of a whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# test each kind of tree\n",
    "for criterion in tree_criterions:\n",
    "    for max_depth in tree_max_depths:\n",
    "        for min_samples_leaf in tree_min_samples_leafs:\n",
    "            \n",
    "            # on each date range\n",
    "            period_pred_Y = []\n",
    "            period_test_Y = []\n",
    "            for i in range(len(final['day_index'].unique()) - days_to_train):\n",
    "\n",
    "                train_df = final[(final['day_index'] >= i) & (final['day_index'] < (i+days_to_train))]\n",
    "                train_Y = train_df['label'].values.tolist()\n",
    "                train_X = train_df.drop(['close', 'high', 'low', 'vol', 'impact', 'label','day_index'], axis=1).values\n",
    "\n",
    "                test_df = final[final['day_index'] == (i + days_to_train)]\n",
    "                if not test_df.empty:\n",
    "                    test_Y = test_df['label'].values.tolist()\n",
    "                    test_X = test_df.drop(['close', 'high', 'low', 'vol', 'impact', 'label','day_index'], axis=1).values\n",
    "\n",
    "                    clf = ExtraTreesClassifier(n_estimators=30)\n",
    "                    clf = clf.fit(train_X, train_Y)\n",
    "\n",
    "                    # pick only those features (technical indicators) which hase at least mean influence on y label\n",
    "                    model = SelectFromModel(clf, prefit=True, threshold=\"0.3*mean\")\n",
    "\n",
    "                    train_X = train_X[:, model.get_support()]\n",
    "                    test_X = test_X[:, model.get_support()]\n",
    "\n",
    "        \n",
    "                    clf = tree.DecisionTreeClassifier(criterion=criterion,\n",
    "                                         max_depth=max_depth,\n",
    "                                         min_samples_leaf=min_samples_leaf)\n",
    "                    clf.fit(train_X, train_Y)\n",
    "                    pred_Y = clf.predict(test_X)\n",
    "                    \n",
    "                    # add predictions for each 'subperiod'\n",
    "                    for i in range(len(pred_Y)):\n",
    "                        period_pred_Y.append(pred_Y[i])\n",
    "                        period_test_Y.append(test_Y[i])\n",
    "\n",
    "        row = []\n",
    "        row.append(i)\n",
    "        row.append(criterion)\n",
    "        row.append(max_depth)\n",
    "        row.append(min_samples_leaf)\n",
    "        row.append(clf)\n",
    "        # dummy score for whole dataframe\n",
    "        good = 0\n",
    "        bad = 0\n",
    "        for i in range(len(period_pred_Y)):\n",
    "            if period_pred_Y[i] == period_test_Y[i] and period_pred_Y[i] != 0:\n",
    "                good = good + 1\n",
    "            if period_pred_Y[i] != period_test_Y[i] and period_pred_Y[i] != 0:\n",
    "                bad = bad + 1\n",
    "        \n",
    "        if good+bad != 0:\n",
    "            row.append(good/(good+bad))\n",
    "        else:\n",
    "            row.append(0)\n",
    "        row.append(period_pred_Y)\n",
    "        row.append(period_test_Y)\n",
    "        df_list.append(row)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(df_list, columns=['day_index','criterion','max_depth','min_samples_leaf','clf','score','pred_Y', 'test_Y'])\\\n",
    "    .sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here are the best classifiers, where \"the best\" means that it had just best direction prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
